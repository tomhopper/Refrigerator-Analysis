---
title: "Refrigerator Analysis"
author: "Thomas Hopper"
date: '`r format(Sys.Date(), "%B %Y")`'
output: 
  html_document: 
    css: ~/Dropbox/R and RMD CSS and HTML headers-footers/rmd_doc_suffix.css
    highlight: tango
    theme: spacelab
references:
- id: r2018
  title: R A Language and Environment for Statistical Computing}
  author: R Core Team
  organization: R Foundation for Statistical Computing
  address: Vienna, Austria
  year: 2018
  url: 'https://www.R-project.org/'
- id: wickham2016
  author: Hadley Wickham
  title: ggplot2 Elegant Graphics for Data Analysis
  publisher: Springer-Verlag New York
  year: 2016
  isbn: 978-3-319-24277-4
  url: 'http://ggplot2.org'


---

```{r setup, include=FALSE}
library(tidyverse)
library(lubridate)
library(qcc)
library(ggrepel)
library(knitr)
library(kableExtra)
library(captioner)

opts_chunk$set(dev="png", 
               dev.args=list(type="cairo"),
               dpi=96)
```

```{r setup_captions, include=FALSE}
fig_nums <- captioner(prefix = "Figure")
fig_ts_raw_cap <- fig_nums(name = "fig_ts_raw",
                           caption = "Time series plot of the data.")
fig_ts_w_cap <- fig_nums(name = "fig_ts_w",
                         caption = "The average daily kWh used by date.")
fig_imr_cap <- fig_nums(name = "fig_imr",
                        caption = "Individuals and moving range charts for the daily consumption data.")
table_nums <- captioner(prefix = "Table")
table_wdata_cap <- table_nums(name = "tab_wdata",
                              caption = "The data set after wrangling.")
```

## The Problem

<span class="newthought">Guy has been measuring his refrigerator's electricity consumption</span> with a power meter, and he wants to know if it's time to replace the refrigerator based on electricity consumption.

The nameplate states that the unit will consume 296 kWh per year. We don't know if this is a target mean, an upper tolerance limit, or the absolute maximum.

## The Data



```{r load_data}
# The data ##
rf_df <- structure(list(Date = c("16/06/18", "29/06/18", "01/07/18", "06/07/18",
                                 "08/07/18", "13/07/18", "15/07/18", "21/07/18", 
                                 "22/07/18", "27/07/18",
                                 "03/08/18", "05/08/18", "10/08/18"), 
                        kWh = c(0, 6.32, 7.97, 12.07,
                                13.9, 0.4, 2.09, 6.42, 7.77, 
                                1.29, 6.73, 8.57, 12.42), 
                        Comment = c(NA, "PC", NA, NA, NA, "PC", NA, NA, NA, 
                                    "PC", NA, NA, NA)), 
                   .Names = c("Date", "kWh", "Comment"), 
                   class = c("tbl_df", "tbl", "data.frame"), 
                   row.names = c(NA, -13L), 
                   spec = structure(
                     list(cols = structure(
                       list(Date = structure(list(),
                                             class = c("collector_character",
                                                       "collector")), 
                            kWh = structure(list(), 
                                            class = c("collector_double",
                                                      "collector")), 
                            Comment = structure(list(),
                                                class = c("collector_character",
                                                          "collector"))), 
                       .Names = c("Date", "kWh", "Comment")), 
                       default = structure(list(), 
                                           class = c("collector_guess",
                                                     "collector"))), 
                     .Names = c("cols", "default"), 
                     class = "col_spec"))

head(rf_df)
```

The power meter reports in cumulative kWh of electricity consumed. However, it resets every time there's a power outage. The the time of the power outages is not known, but the first measurement after a power outage is marked with a "PC" in the $Comment$ variable.

## EDA

Here's how the data looks in raw form, plotted as a simple time series

```{r eda, fig.cap=fig_ts_raw_cap}
# Preliminary EDA ##
rf_df %>%
  mutate(index = row_number()) %>% 
  ggplot(aes(x = index, y = kWh)) +
  geom_point() +
  geom_label_repel(aes(label = Comment))
```

The cumulative nature of the reported energy consumption, and the occasional resets, are both obvious.

It's also clear that the consumption is not entirely linear&mdash;there is some variation in the rate of use from day to day.

Of note: the first data point precedes the first power outage.

## Data wrangling

To predict annual electricity usage, we need to calculate a rate that we can then annualize. We can use the given cumulative numbers to derive an average daily rate between each observation. As we don't know when each reset point was, we have to be careful about how we use these values. For instance, we want to eliminate negative rates ("drops" in cumulative kWh due to a reset), and we can't make assumptions about when the power outages occur (i.e. we don't know where the '0's are).

```{r wrangle}
# Wrangle the data ##
# Fix the dates so we can do something useful
# Calculate the kWh used between each observation
# Calculate the days between observations
rf_df <- rf_df %>%
  mutate(Date = dmy(Date)) %>% 
  mutate(diff = kWh - lag(kWh),
         days = Date - lag(Date)) 

# Due to power outages, lines with "PC" have invalid $diff values, 
# so convert to NA
# Had some trouble doing this in dplyr due to the NAs...
rf_df$diff[rf_df$Comment == "PC" & !is.na(rf_df$Comment)] <- NA

# Now drop the Comment column, and then drop any rows 
# with NA values.
# This leaves us with useful observations of 
# kWh used between dates. 
# Now we calculate the average daily
# kWh consumption
rf_df <- rf_df %>% 
  select(-Comment) %>% 
  na.omit() %>% 
  mutate(daily_kWh = diff / as.integer(days))
```

Let's have alook at the full, wrangled data set.

<span class="caption">`r table_nums('tab_wdata')`</span>
```{r show_data, results="asis", echo=FALSE}
kable(rf_df, format = "html") %>% kable_styling()
```

The key variable that we're interested in is the $daily\_kWh$, so let's also plot it.

```{r analysis, fig.height=3, fig.width=5, fig.cap=fig_ts_w_cap}
# Analysis ##
# Basic time series plot of the data
rf_df %>% 
  ggplot(aes(x = Date, y = daily_kWh)) +
  geom_point()
```

## Analysis and Results

Our analysis will check that the data is suitable for making predictions. We will then predict a range of possible annual values, and finally assess whether the specification of 296 kWh per annum falls with those predictions.

In `r fig_nums("fig_ts_w", display = "cite")` there seems to be an unusually high (and maybe a low) value. Is this an outlier? Is it an artifact of the measurement process? Was Guy having a party and the refrigerator was seeing heavy use?

These questions are critical to our ability to reliably predict future data. We have sampled some process. In order to make predictions about the future, we need those samples to all come from a singular, or *homogeneous*, process. If we're seeing samples different processes, or *inhomogeneous*, processes then we either have to segregate the data or give up making predictions.

We can use an individuals and range chart ("ImR") to assess whether this data exhibits homogeneous properties&mdash;are we sampling from a single population of data and can make predictions about the future?

```{r qcc, fig.height=3.5, fig.width=6, fig.cap=fig_imr_cap}
# Use qcc to plot the individual values and the moving range
rf_imr <- qcc(data = rf_df$daily_kWh, type = "xbar.one")
# Create the matrix used by qcc for the moving range chart.
# Column 1 is the individuals data; column 2 is the same data with
# a 1-row lag.
rf_daily_m <- matrix(c(rf_df$daily_kWh, lag(rf_df$daily_kWh)), 
                     byrow = FALSE, ncol = 2)
rf_r <- qcc(data = rf_daily_m, type = "R", sizes = 2)
```

With no out-of-control points on either the individuals or the moving range chart, it appears that we do have a homogeneous sample, and can continue with a prediction. However, nine sample points are not really enough to make this assessment with a high degree of confidence; we should continue collecting data until we have between 20 and 30 observations in our cleaned data set.[^1]

Given the priso above, we now have estimates for key population parameters.

```{r pop_params}
# Get the mean and standard deviation from these plots
rf_daily_mean <- rf_imr$center
rf_daily_sd <- rf_r$center
```

These show an average daily consumption of `r format(round(rf_daily_mean, 2), digits = 2)` kWh, with a standard deviation of `r format(round(rf_daily_sd, 2), digits = 2, nsmall = 2)` kWh.

We use Monte Carlo simulation to estimate annual electricity use for 1000 identical cases.

```{r mc_sim}
# Use Monte Carlo simulation with these values to simulate
# 1000 years of operation of this refigerator (or, alternately,
# 1000 identical refrigerators operating for a year)
rf_annual <- replicate(expr = rnorm(n = 365, 
                                    mean = rf_daily_mean, 
                                    sd = rf_daily_sd) %>% 
                         sum(), 
                       n = 1000)

head(rf_annual)
```

```{r tols}
# How big do we want our prediction intervals?
sigmas <- 2

# Percent of cases covered by prediction interval
pi_percent_data <- paste0(format(x = (pnorm(sigmas) - pnorm(-sigmas)) * 100, 
                                 digits = 2, 
                                 nsmall = 0), 
                          "%")
```

Using the above Monte Carlo simulation, we can create tolerance intervals to bracket the consumption values that we will likely see over a year's time. In fact, we will construct a `r sigmas`-sigma, or `r pi_percent_data`, tolerance interval with 50% confidence.[^2]

```{r story}
# Data Story ##
# Population mean and standard deviation, based on the moving range
rf_annual_mean <- mean(rf_annual)
rf_annual_sd <- sd(rf_annual)
# Prediction interval lower and upper
lcl <- rf_annual_mean - sigmas * rf_annual_sd
ucl <- rf_annual_mean + sigmas * rf_annual_sd


rf_annual_mean_pt <- format(round(mean(rf_annual), 0), digits = 2)
rf_annual_sd_pt <- format(round(sd(rf_annual), 1), digits = 2)
rf_annual_lcl <- format(lcl, digits = 3)
rf_annual_ucl <- format(ucl, digits = 3)

compare_txt <- if(lcl > 296) {
  c("more than")
} else {
  if(ucl < 296) {
    c("less than")
  } else {
    c("about the same as")
  }
}
```

## Conclusion

About `r pi_percent_data` of cases fall between `r rf_annual_lcl` kWh and `r rf_annual_ucl` kWh per annum with a 50% confidence. With a nameplate consumption of 296 kWh per annum, we can say that the refrigerator consumes `r compare_txt` the nameplate.


[^1]: “How Much Data Do I Need to Calculate Control Limits?” *Control Charts Basics* | BPI Consulting, Aug. 2016, www.spcforexcel.com/knowledge/control-chart-basics/how-much-data-do-i-need-calculate-control-limits.

[^2]: Wheeler, Donald J. “Statistical Tolerance Intervals.” *Quality Digest*, 4 Jan. 2016, www.qualitydigest.com/inside/statistics-column/010416-statistical-tolerance-intervals.html.

# References